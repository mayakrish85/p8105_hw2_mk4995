---
title: "P8105 Homework 2"
author: "Maya Krishnamoorthy"
date: "2024-09-25"
output: github_document
---

``` {r setup, echo=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
```

## Problem 1

This problem focuses on NYC transit data. The dataset `NYC_Transit_Subway_Entrance_and_Exit_Data.csv` contains information related to each entrance and exit for each subway station in NYC.

###### Clean the data.
``` {r message=FALSE}
# Step 1: Read and clean the data: retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance. Convert the entry variable from character (YES vs NO) to a logical variable (the ifelse or case_match function may be useful).

nyc_transit = 
  read_csv("data/NYC_Transit_Subway_Entrance_and_Exit_Data.csv") |> 
  janitor::clean_names() |> 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) |> 
  mutate(
    entry = case_match(
      entry, 
      "YES" ~ TRUE, 
      "NO" ~ FALSE)
  )
```

###### Dataset Desciption

The dataset `NYC_Transit_Subway_Entrance_and_Exit_Data.csv` originally contains 32 columns and 1,868 rows. The variables include the line, name, station and entrance location by latitude and longitude, entrance type, ADA compliance, routes served, cross streets, vending availability, and more. The cleaned dataset `nyc_transit` contains a subset of the original dataset (with only 19 columns). `nyc_transit` variables include the line, station name, station latitude and longitude, routes served, entry, vending, entrance type, and ADA compliance. 

To clean the data, I started by cleaning the names, and then selected only the necessary columns as noted above. I then used the mutate function to convert the entry variable from a character type to a boolean type. These data are not tidy because there is redundancy in the columns (route1:route11).

###### Answer questions about data.

``` {r message=FALSE}
# How many distinct stations are there?
distinct_stations = 
  nyc_transit |> 
  distinct(station_name, line) |> 
  nrow()

# How many stations are ADA compliant?
ada_compliant = 
  nyc_transit |> 
  filter(ada==TRUE) |> 
  distinct(station_name, line) |> 
  nrow()

# What proportion of station entrances / exits without vending allow entrance?
prop_vending_entrance = 
  nyc_transit |> 
  summarize(
    proportion = mean(vending == "NO" & entry == TRUE)
  )
```

There are `r distinct_stations` distinct stations. Out of those, `r ada_compliant` stations are ADA compliant. The proportion of station entrances / exits without vending that allow entrance is `r prop_vending_entrance`.


*I'm not describing the output because I would like to put that time towards problems 2 and 3.*
``` {r, message=FALSE}
# Reformat data so that route number and route name are distinct variables.
reformatted_nyc_transit = 
  nyc_transit |> 
  mutate(
    across(route1:route11, as.character)
  ) |> 
  pivot_longer(
    cols = route1:route11,
    names_to = "route_number",
    names_prefix = "route",
    values_to = "available_route"
  )

# How many distinct stations serve the A train?
A_train_service = 
  reformatted_nyc_transit |> 
  filter(available_route == "A") |> 
  distinct(station_name, line)

nrow(A_train_service)

# Of the stations that serve the A train, how many are ADA compliant?
A_train_ADA =
  reformatted_nyc_transit |> 
  filter(ada == TRUE & available_route == "A") |> 
  distinct(station_name, line)
  
nrow(A_train_ADA)
```


## Problem 2

###### Read and clean each sheet from the Trash Wheel Excel sheet.

``` {r message=FALSE}
# Read trash data as a function for all the common steps in each sheet
read_sheet_data = function(sheet_name) {
  sheet_data = 
    read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
               sheet = sheet_name, # for each sheet
               na = c("NA", ".", ""), # reassign NA columns
               skip = 1 # skip first row because it's a picture
    ) |> 
    janitor::clean_names() |>
    select(!homes_powered) # Remove columns that are not directly related to dumpster data for all sheets
  
  return(sheet_data)
}

# Read Mr. Trash Wheel Data
mr_trash_wheel = read_sheet_data("Mr. Trash Wheel") |> 
  select(!x15:x16) |> # Remove columns with only NA
  mutate(
    sports_balls = as.integer(round(sports_balls)), # Round number of sports balls to the closest integer
    year = as.numeric(year), # convert year to a numeric variable
    trash_wheel = "Mr. Trash Wheel" # create a new column that tells us which sheet this data represents (for binding later)
  )

# Read Professor Trash Wheel Data
professor_trash_wheel = read_sheet_data("Professor Trash Wheel") |> 
  mutate(trash_wheel = "Professor Trash Wheel") # new column for binding

# Read Gwynnda Trash Wheel Data
gwynnda_trash_wheel = read_sheet_data("Gwynnda Trash Wheel") |> 
  mutate(trash_wheel = "Gwynnda Trash Wheel") # new column for binding
```

###### Merge the datasets.

``` {r}
trash_wheel_df = 
  bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda_trash_wheel) |> # bind the tables
  relocate(trash_wheel) # move the trash_wheel variable to the first column so we can see which trash wheel we are looking at
```

###### Summarize the data and answer some questions.

``` {r}
# Count subset of trash wheel dataset to better describe the data
n_mr_trash = 
  trash_wheel_df |> 
  filter(trash_wheel == "Mr. Trash Wheel") |> 
  nrow()

n_prof_trash = 
  trash_wheel_df |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  nrow()

n_gwynnda_trash = 
  trash_wheel_df |> 
  filter(trash_wheel == "Gwynnda Trash Wheel") |> 
  nrow()
```

Taking a deeper dive into `trash_wheel_df`, we can see that the table overall contains 14 columns and 849 rows. `r n_mr_trash` rows contain data from the `Mr. Trash Wheel` subset, 107 rows describe `r n_prof_trash`, and `r n_gwynnda_trash` rows describe `Gwynnda Trash Wheel.` Key variables include `weight_tons` and `volume_cubic_yards`, as they tell us the overall volume of the trash collected in a day. `plastic_bottles`, `polystyrene` `cigarette_butts`, `glass_bottles`, `plastic_bags`, `wrappers`, and `sports_balls` are helpful in telling us the volume of trash collected that belonged to each of those categories. 

``` {r}
# Answering specific questions! 

total_prof_trash_weight = # calculate the total trash weight in tons for Prof Trash Wheel
  trash_wheel_df |> 
  filter(trash_wheel == "Professor Trash Wheel") |> 
  summarise(total_weight = sum(weight_tons, na.rm = TRUE))

total_gwynnda_cigs = # calculate total number of cigarette butts collected by Gwynnda in June of 2022
  trash_wheel_df |> 
  filter(
    trash_wheel == "Gwynnda Trash Wheel" & 
      year == 2022 &
      month == "June") |> 
  summarise(total_cigarette_buts = sum(cigarette_butts, na.rm = TRUE))
```

The total weight in tons of the trash collected by Professor Trash Wheel is `r total_prof_trash_weight`. A total of `r total_gwynnda_cigs` cigarette butts was collected by Gwynnda Trash Wheel in June 2022.


## Problem 3

Create a single, well-organized dataset with the information in the Great British Bake-Off files.

###### Examine and clean `bakes.csv`, `bakers.csv`, and `results.csv`.

```{r message=FALSE}

bakers_df = read_csv(
  "data/gbb_datasets/bakers.csv"
  ) |> 
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker_first_name", "baker_last_name"), sep = " ")

bakes_df = read_csv("data/gbb_datasets/bakes.csv", na = c("N/A", "NA", ".", "UNKNOWN")) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(baker_first_name = str_replace_all(baker_first_name, '["\']', ""))

results_df = read_csv("data/gbb_datasets/results.csv", skip = 2) |> 
  janitor::clean_names() |> 
  rename(baker_first_name = baker) |> 
  mutate(baker_first_name = str_replace_all(baker_first_name, '["\']', ""))
```


We can see that `bakers.csv` does not contain the `episode` column. After importing each dataset, I noticed that the `baker_name` column contained the first and last name of each baker in `bakers_df`, whereas `bakes_df` and `results_df` contained only the first name in the `baker` column. To make the merge process easier, I separated the first and last name columns in `bakers_df`. I also converted thet `baker` column to `baker_first_name` and removed any excess quotations in the names (i.e. "Jo" to Jo).

###### Merge the datasets.

*Now start by merging results_df and bakes_df.*

```{r}
# Using anti-join
anti_join_bakes_results = anti_join(bakes_df, results_df)

# First merge results and bakes using the variables identified in the anti_join.
bake_results = right_join(
  bakes_df, 
  results_df, 
  by = c("series", "episode", "baker_first_name")
  )
```

I first used `anti_join()` to check what variables to merge `bakes_df` and `results_df`. Then I used a left join to put the datasets together on "series", "episode", and "baker_first_name."

*Merge the resulting dataset with bakers_df.*

```{r}
# Now find the variables that can join the two datasets.
anti_join_bakes_results = anti_join(bakers_df, bake_results)

# Merge the dataframes by series and baker_first_name.
merged_df = right_join(
  bakers_df,
  bake_results,
  by = c("series", "baker_first_name")
)

# Not sure if this is tidy - redundancy in the names?
gbb_tidy_df =
  merged_df |> 
  unite(
    "baker", 
    baker_first_name, 
    baker_last_name, 
    sep = " ", 
    na.rm = TRUE
  ) |> 
  filter(!is.na(result)) |> 
  relocate(series, episode, baker) |> 
  arrange(series, episode)
```

The resulting dataset, `gbb_tidy_df` contains the three merged datasets, which I joined on "series" and "baker_first_name." Then, I cleaned the dataset by putting the first and last name columns back together. I also removed rows where the results were not available, a.k.a. the contestant did not reach that round. Lastly, I relocated the columns "series" and "episodes" to the head of the dataset and arranged the dataset in ascending order of series and episodes. The resulting dataset has `r nrow(gbb_tidy_df)` rows and `r ncol(gbb_tidy_df)` columns (variables).

I'm not sure if this data can still be considered tidy since the names of each baker and their hometown is repeated multiple times. Is there a way to further reorganize and clean the data? Also, can I drop rows based on that column? I think that logic may be correct, but I'm not sure.

*Export the dataset.*
```{r}
write_csv(gbb_tidy_df, "data/gbb_datasets/gbb_tidy_df.csv")
```

###### Answer questions.

## something's wrong -- what's happening in signature bake???
```{r}
winners_seasons5_10 = 
  gbb_tidy_df |> 
  filter(
    series >= 5,
    result == "WINNER"
  )

knitr::kable(winners_seasons5_10)
```

The technical score of the winners of season 7, 8, and 9 is 2 (not the best overall score), so it's surprising that they won. The other winners were as expected.

###### Work on `viewers.csv`



